---
title: "Classification"
author: "JACE HIGA!"
date: "02/24/2025"

format: 
  html: 
    theme: superhero  
    mainfont: monospace
    highlight-style: github
    title-block-banner: true
    embed-resources: true
---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](https://raw.githubusercontent.com/cd-public/D505/refs/heads/master/hws/src/classify.qmd) hosted on GitHub pages.

# 0. Quarto Type-setting

- This document is rendered with Quarto, and configured to embed an images using the `embed-resources` option in the header.
- If you wish to use a similar header, here's is the format specification for this document:

```email
format: 
  html:
    embed-resources: true
```

# 1. Setup

**Step Up Code:**

```{r}
sh <- suppressPackageStartupMessages
sh(library(tidyverse))
sh(library(caret))
sh(library(naivebayes))
wine <- readRDS(gzcon(url("https://github.com/cd-public/D505/raw/master/dat/pinot.rds")))
```

# 2. Logistic Concepts

Why do we call it Logistic Regression even though we are using the technique for classification?
> <span style="color:red;font-weight:bold">TODO</span>: *It is because the basis of it comes from using linear combinations of our chosen features to help in modeling relationships in our data. However, it also  uses the logistic function to also handle categorical outcomes which allows us to be able to classify.*

# 3. Modeling

We train a logistic regression algorithm to classify a whether a wine comes from Marlborough using:

1. An 80-20 train-test split.
2. Three features engineered from the description
3. 5-fold cross validation.

We report Kappa after using the model to predict provinces in the holdout sample.


```{r}
library(tidytext)
wine_desc <- wine %>%
  filter(province == "Marlborough") %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  arrange(desc(n))
```

```{r}
library(SnowballC)
word <- wine_desc %>%
  mutate(word = wordStem(word))
```

```{r}
wino <- wine %>%
  mutate(drink = str_detect(description, "drink")) %>%
  mutate(cherri = str_detect(description, "cherri")) %>%
  mutate(fruit = str_detect(description, "fruit")) %>%
  select(-description)
```

```{r}
set.seed(5)

wine_index <- createDataPartition(wino$province, p = 0.8, list = FALSE)
train <- wino[wine_index, ]
test <- wino[-wine_index, ]

train$province <- factor(train$province)
test$province <- factor(test$province, levels = levels(train$province))
```

```{r}
train_control <- trainControl(method = "cv", number = 5)

get_fit <- function(train) {
  train(province ~ .,
        data = train, 
        trControl = train_control,
        method = "multinom",
        maxit = 5)}

fit <- get_fit(train)
```

```{r}
fit
```


# 4. Binary vs Other Classification

What is the difference between determining some form of classification through logistic regression versus methods like $K$-NN and Naive Bayes which performed classifications.

> <span style="color:red;font-weight:bold">TODO</span>: *The biggest difference is that regression is able to weigh the features, in our case descriptive words, based on how often they appear.*


# 5. ROC Curves

We can display an ROC for the model to explain your model's quality.

```{r}
desc_to_words <- function(df, omits) { 
  df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% omits))
}

words_to_stems <- function(df) { 
  df %>%
    mutate(word = wordStem(word))
}

filter_by_count <- function(df, j) { 
  df %>%
    count(id, word) %>% 
    group_by(id) %>% mutate(exists = (n>0)) %>% ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j)
}

pivoter <- function(words, df) {
  words %>%
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% 
    drop_na() %>% 
    select(-id)
}
```

```{r}
wine_words <- function(df, j, stem) { 

  words <- desc_to_words(df, c("drink","cherri","fruit"))
  
  if (stem) {
    words <- words_to_stems(words)
  }
  
  words <- filter_by_count(words, j)

  pivoter(words, df)
}
```



```{r}
# You can find a tutorial on ROC curves here: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5
```

```{r}
winor <- wine_words(wine, 1000, T) %>% 
           mutate(marlborough = as.factor(province == "Marlborough")) %>%
           select(-province)


wine_index <- createDataPartition(winor$marlborough, p = 0.80, list = FALSE)
train <- winor[wine_index, ]
test <- winor[-wine_index, ]



fit <- train(marlborough ~ .,
             data = train, 
             trControl = train_control,
             method = "glm",
             family = "binomial")
```

```{r}
library(pROC)

prob <- predict(fit, newdata = test, type = "prob")[,2]
myRoc <- roc(test$marlborough, prob)
plot(myRoc)
```

```{r}
auc(myRoc)
```

> <span style="color:red;font-weight:bold">TODO</span>: *It has an ROC of 0.9045 which is considered to be fairly good.*



